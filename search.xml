<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[JVM常见问题QA]]></title>
    <url>%2F2019%2F11%2F15%2Fjvm-qa%2F</url>
    <content type="text"><![CDATA[什么是类的加载 ? 类的加载指的是将类的.class文件中的二进制数据读入到内存中，将其放在运行时数据区的方法区内，然后在堆区创建一个java.lang.Class对象，用来封装类在方法区内的数据结构。类的加载的最终产品是位于堆区中 的Class对象，Class对象封装了类在方法区内的数据结构，并且向Java程序员提供了访问方法区内的数据结构的接口。 类的生命周期 类的生命周期包括这几个部分，加载、连接、初始化、使用和卸载，其中前三部是类的加载的过程,如下图: 加载，查找并加载类的二进制数据，在Java堆中也创建一个java.lang.Class类的对象 连接，连接又包含三块内容:验证、准备、初始化。 1)验证，文件格式、元数据、字节码、符号引用验证 2)准备，为类的静态变量分配内存，并将其初始化为默认值 3)解析，把类中的符号引用转换为直接引用 初始化，为类的静态变量赋予正确的初始值 使用，new出对象程序中使用 卸载，执行垃圾回收 类加载器 启动类加载器: Bootstrap ClassLoader，负责加载存放在JDK\jre\lib(JDK代表JDK的安装目录，下同)下，或被-Xbootclasspath参数指定的路径中的，并且能被虚拟机识别的类库 扩展类加载器:Extension ClassLoader，该加载器由sun.misc.Launcher$ExtClassLoader实现，它负责加载JDK\jre\lib\ext目录中，或者由java.ext.dirs系统变量指定的路径中的所有类库(如javax.*开头的类)，开发者可以直接使用扩展类加载器。 应用程序类加载器:Application ClassLoader，该类加载器由sun.misc.Launcher$AppClassLoader来实现，它负责加载用户类路径(ClassPath)所指定的类，开发者可以直接使用该类加载器 类加载机制 全盘负责，当一个类加载器负责加载某个Class时，该Class所依赖的和引用的其他Class也将由该类加载器负责载入，除非显示使用另外一个类加载器来载入 父类委托，先让父类加载器试图加载该类，只有在父类加载器无法加载该类时才尝试从自己的类路径中加载该类 缓存机制，缓存机制将会保证所有加载过的Class都会被缓存，当程序中需要使用某个Class时，类加载器先从缓存区寻找该Class，只有缓存区不存在，系统才会读取该类对应的二进制数据，并将其转换成Class对象，存入缓存区。这就是为什么修改了Class后，必须重启JVM，程序的修改才会生效 双亲委派模型 当一个类收到了类加载请求时，不会自己先去加载这个类，而是将其委派给父类，由父类去加载，如果此时父类不能加载，反馈给子类，由子类去完成类的加载。 自底向上的检查，自顶向下的加载 注意双亲委派模式的问题:无法识别应用程序类加载器中的类解决方案:设置一个上下文加载器角色解决 JVM内存结构 方法区和堆是所有线程共享的内存区域，而java栈、本地方法栈和程序计数器是运行是线程私有的内存区域。 Java堆(Heap)，是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内 存。 方法区(Method Area)，方法区(Method Area)与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 程序计数器(Program Counter Register)，程序计数器(Program Counter Register)是一块较小的内存空间，它的作用可以看做是当前线程所执行的字节码的行号指示器。 JVM栈(JVM Stacks)，与程序计数器一样，Java虚拟机栈(Java Virtual Machine Stacks)也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型:每个方法被执行的时候都会同时创建一个栈帧(Stack Frame)用于存储局部变量表、操作栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 本地方法栈(Native Method Stacks)，本地方法栈(Native Method Stacks)与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java方法(也就是字节码)服务，而本地方法栈则是为虚拟机使用到的Native方法服务。 对象分配规则 对象优先分配在Eden区，如果Eden区没有足够的空间时，虚拟机执行一次Minor GC。 大对象直接进入老年代(大对象是指需要大量连续内存空间的对象)。这样做的目的是避免在Eden区和两个Survivor区之间发生大量的内存拷贝(新生代采用复制算法收集内存)。 长期存活的对象进入老年代。虚拟机为每个对象定义了一个年龄计数器，如果对象经过了1次Minor GC那么对象会进入Survivor区，之后每经过一次Minor GC那么对象的年龄加1，知道达到阀值对象进入老年区。 动态判断对象的年龄。如果Survivor区中相同年龄的所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象可以直接进入老年代。 空间分配担保。每次进行Minor GC时，JVM会计算Survivor区移至老年区的对象的平均大小，如果这个值大于老年区的剩余值大小则进行一次Full GC，如果小于检查HandlePromotionFailure设置，如果true则只进行Monitor GC,如果false则进行Full GC。 GC算法 垃圾回收 对象存活判断 判断对象是否存活一般有两种方式: 引用计数: 每个对象有一个引用计数属性，新增一个引用时计数加1，引用释放时计数减1，计数为0时可以回收。此方法简单，无法解决对象相互循环引用的问题。 可达性分析(Reachability Analysis): 从GC Roots开始向下搜索，搜索所走过的路径称为引用链。当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的，不可达对象。 GC算法 GC最基础的算法有三种:标记 -清除算法、复制算法、标记-压缩算法，我们常用的垃圾回收器一般都采用分代收集算法。 标记 -清除算法，“标记-清除”(Mark-Sweep)算法，如它的名字一样，算法分为“标记”和“清除”两个阶段:首先标记出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象。 复制算法，“复制”(Copying)的收集算法，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的 内存空间一次清理掉。 标记-压缩算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存 分代收集算法，“分代收集”(Generational Collection)算法，把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。 垃圾回收器 Serial收集器，串行收集器是最古老，最稳定以及效率高的收集器，可能会产生较长的停顿，只使用一个线程去回收。 ParNew收集器，ParNew收集器其实就是Serial收集器的多线程版本。 Parallel收集器，Parallel Scavenge收集器类似ParNew收集器，Parallel收集器更关注系统的吞吐量。 Parallel Old 收集器，Parallel Old是Parallel Scavenge收集器的老年代版本，使用多线程和“标记-整理”算法 CMS收集器，CMS(Concurrent Mark Sweep)收集器是一种以获取最短回收停顿时间为目标的收集器。 G1收集器，G1 (Garbage-First)是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足GC停顿时间要求的同时,还具备高吞吐量性能特征 GC分析 命令调优 GC日志分析 摘录GC日志一部分(前部分为年轻代gc回收;后部分为full gc回收): 通过上面日志分析得出，PSYoungGen、ParOldGen、PSPermGen属于Parallel收集器。其中PSYoungGen表示gc回收前后年轻代的内存变化;ParOldGen表示gc回收前后老年代的内存变化;PSPermGen表示gc回收前后永 久区的内存变化。young gc 主要是针对年轻代进行内存回收比较频繁，耗时短;full gc 会对整个堆内存进行回城，耗时长，因此一般尽量减少full gc的次数 young gc 日志: Full GC日志: 调优命令 Sun JDK监控和故障处理命令有jps jstat jmap jhat jstack jinfo jps，JVM Process Status Tool,显示指定系统内所有的HotSpot虚拟机进程。 jstat，JVM statistics Monitoring是用于监视虚拟机运行时状态信息的命令，它可以显示出虚拟机进程中的类装载、内存、垃圾收集、JIT编译等运行数据。 jmap，JVM Memory Map命令用于生成heap dump文件 jhat，JVM Heap Analysis Tool命令是与jmap搭配使用，用来分析jmap生成的dump，jhat内置了一个微型的HTTP/HTML服务器，生成dump的分析结果后，可以在浏览器中查看jstack，用于生成java虚拟机当前时刻的线程快照。 jinfo，JVM Configuration info 这个命令作用是实时查看和调整虚拟机运行参数。 Jvm调优-命令篇调优工具 常用调优工具分为两类,jdk自带监控工具:jconsole和jvisualvm，第三方有:MAT(Memory Analyzer Tool)、GChisto。 jconsole，Java Monitoring and Management Console是从java5开始，在JDK中自带的java监控和管理控制台，用于对JVM中内存，线程和类等的监控 jvisualvm，jdk自带全能工具，可以分析内存快照、线程快照;监控内存变化、GC变化等。 MAT，Memory Analyzer Tool，一个基于Eclipse的内存分析工具，是一个快速、功能丰富的Java heap分析工具，它可以帮助我们查找内存泄漏和减少内存消耗 GChisto，一款专业分析gc日志的工具 你知道哪些JVM性能调优 设定堆最小内存大小-Xms -Xmx:堆内存最大限制。 设定新生代大小。 新生代不宜太小，否则会有大量对象涌入老年代 -XX:NewSize:新生代大小 -XX:NewRatio 新生代和老生代占比 -XX:SurvivorRatio:Eden区与每一个Survivor区的比值 设定垃圾回收器 年轻代用 -XX:+UseParNewGC (串行) 年老代用-XX:+UseConcMarkSweepGC (CMS) 设定锁的使用 多线程下关闭偏向锁，比较浪费资源 g1 和 cms 区别 吞吐量优先和响应优先的垃圾收集器选择 CMS是一种以最短停顿时间为目标的收集器 响应优先选择CMS,吞吐量高选择G1 当出现了内存溢出，怎么排错用jmap看内存情况，然后用 jstack主要用来查看某个Java进程内的线程堆栈信息 然后看报错的信息信息定位发生OOM的位置:堆，java虚拟机栈，永久区，直接内存 然后根据具体的问题提出具体的解决方案 JVM的一些参数设定 JVM调优 偏向锁]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java常见问题QA]]></title>
    <url>%2F2019%2F11%2F15%2Fjava-qa%2F</url>
    <content type="text"><![CDATA[Java的HashMap是如何工作的? HashMap是一个针对键值的数据结构，每个键都会有相应的值。HashMap 基于 hashing 原理，我们通过 put ()和 get ()方法储存和获取对象。当我们将键值对传递给 put ()方法时，它调用键对象的 hashCode ()方法来计算 hashcode，让后找到 bucket 位置来储存 值对象。当获取对象时，通过键对象的 equals ()方法找到正确的键值对，然后返回值对象。HashMap 使用 LinkedList 来解决碰撞问题，当发生碰撞了，对象将会储存在 LinkedList 的下一个节 点中。 HashMap 在每个 LinkedList 节点中储存键值对对象。 什么是快速失败的故障安全迭代器? 快速失败的Java迭代器可能会引发ConcurrentModifcationException在底层集合迭代过程中被修改。故障安全作为发生在实例中的一个副本迭代是不会抛出任何异常的。快速失败的故障安全范例定义了当遭遇故障时系统是如何反应的。例如，用于失败的快速迭代器ArrayList和用于故障安全的迭代器ConcurrentHashMap。 Java BlockingQueue是什么? Java BlockingQueue是一个并发集合util包的一部分。BlockingQueue队列是一种支持操作，它等待元素变得可用时来检索，同样等待空间可用时来存储元素。 什么时候使用ConcurrentHashMap? ConcurrentHashMap被作为故障安全迭代器的一个实例，它允许完整的并发检索和更新。当有大量的并发更新时，ConcurrentHashMap此时可以被使用。这非常类似于Hashtable，但ConcurrentHashMap不锁定整个表来提供并发，所以从这点上ConcurrentHashMap的性能似乎更好一些。所以当有大量更新时ConcurrentHashMap应该被使用。 哪一个List实现了最快插入? LinkedList和ArrayList是两个不同变量列表的实现。ArrayList的优势在于动态的增长数组，非常适合初始时总长度未知的情况下使用。LinkedList的优势在于在中间位置插入和删除操作，速度是最快的。LinkedList实现了List接口，允许null元素。此外LinkedList提供额外的get，remove，insert方法在LinkedList的首部或尾部。这些操作使LinkedList可被用作堆栈(stack)，队列(queue)或双向队列(deque)。ArrayList实现了可变大小的数组。它允许所有元素，包括null。 每个ArrayList实例都有一个容量(Capacity)，即用于存储元素的数组的大小。这个容量可随着不断添加新元素而自动增加，但是增长算法并没有定义。当需要插入大量元素时，在插入前可以调用ensureCapacity方法来增加ArrayList的容量以提高插入效率。 Iterator和ListIterator的区别是什么? ListIterator有add()方法，可以向List中添加对象，而Iterator不能。 ListIterator和Iterator都有hasNext()和next()方法，可以实现顺序向后遍历，但是ListIterator有hasPrevious()和previous()方法，可以实现逆向(顺序向前)遍历。Iterator就不可以。 ListIterator可以定位当前的索引位置，nextIndex()和previousIndex()可以实现。Iterator没有此功能。 都可实现删除对象，但是ListIterator可以实现对象的修改，set()方法可以实现。Iierator仅能遍历，不能修改。 什么是CopyOnWriteArrayList，它与ArrayList有何不同? CopyOnWriteArrayList是ArrayList的一个线程安全的变体，其中所有可变操作(add、set等等)都是通过对底层数组进行一次新的复制来实现的。相比较于ArrayList它的写操作要慢一些，因为它需要实例的快照。 CopyOnWriteArrayList中写操作需要大面积复制数组，所以性能肯定很差，但是读操作因为操作的对象和写操作不是同一个对象，读之间也不需要加锁，读和写之间的同步处理只是在写完后 通过一个简单的”=”将引用指向新的数组对象上来，这个几乎不需要时间，这样读操作就很快很安全，适合在多线程里使用，绝对不会发生ConcurrentModificationException ，因此CopyOnWriteArrayList适合使用在读操作远远大于写操作的场景里，比如缓存。 迭代器和枚举之间的区别 Iterator允许移除从底层集合的元素。 Iterator的方法名是标准化的。 Hashmap如何同步? 当我们需要一个同步的HashMap时，有两种选择: 使用Collections.synchronizedMap(..)来同步HashMap。 使用ConcurrentHashMap IdentityHashMap和HashMap的区别 IdentityHashMap是Map接口的实现。不同于HashMap的，这里采用参考平等。 在HashMap中如果两个元素是相等的，则key1.equals(key2) 在IdentityHashMap中如果两个元素是相等的，则key1 == key2 进程与线程区别以及线程相关概念 进程就是运行中的程序，每个进程占用独自的内存空间 线程属于进程，一个进程可以有一个或多个线程，这些线程共享这个进程的内存或系统资源，线程的切换比进程切换的负担要小。一个Java应用总是从main()方法开始运行，mian()方法运行在一个线程内，它被称为主线程。多线程的最终目的是尽可能的利用cpu资源，不让其闲置。 两种创建线程的方式 (1) 继承Thread类，实现其run方法 123456class Thread1 extends Thread &#123; @Override public void run() &#123; &#125;&#125; (2) 实现Runnale接口，实现run方法 123456class Thread2 implements Runnable &#123; @Override public void run() &#123; &#125;&#125; Thread源代码分析特点 (1)Thread类实现了Runnale接口，实现了其run方法 (2)当生成一个线程对象的时候，如果没有为其设定名字，线程对象将使用如下形式:Thread-number(该number是自动增加的并共享于该类其它对象) (3)两种方法均需要执行start方法分配必须的系统资源，调度线程运行并执行run (4)在具体应用中，采用那种方式看情况而定，但当一个线程继承了另外一个类时，只能实现Runnable接口 线程的生命周期 概念:一个线程从创建到消亡的过程。 状态: (1)创建状态:new—start之间称为创建状态，创建状态的线程是一个空线程系统不为其非配资源。 (2)可运行状态:start–run可运行但并不是一定在运行，只是拥有了运行的条件。 (3)不可运行状态:当调用了sleep方法或者wait方法。在指定时间后恢复或调用notify相关方法恢复。 (4)退出状态:调用了stop方法或者自然消亡。线程的停止:线程的消亡不能通过一个stop()命令，而是让run方法自然结束。可以在while循环里面条件判断break或者return。 线程的优先级 1.线程的优先级及其设置 目的:设置优先级是为了在多线程环境中便于系统对线程的调度，优先级高的线程将优先执行。 原则: —-线程创建时，子继承父的优先级 —-setPriority()方法改变优先级 —-优先级数由低到高是1——10的正整数，默认为5.(动态) 2.线程的调度策略 线程调度器选择优先级最高的线程运行，但是，如果发生以下情况，就会终止线程的运行: (1)线程体中调用了yield方法让出了对cpu的占用权利 (2)线程体中调用了sleep方法使线程进入睡眠状态 (3)线程由于IO操作受到阻塞 (4)另外一个更高优先级线程出现 (5)在支持时间片的系统中，该线程的时间片用完。 关于成员变量和局部变量 如果一个变量是成员变量，那么多个线程对同一个对象的成员变量进行操作的时候，他们对该成员变量是彼此影响的，也就是说一个线程对成员变量的改变会影响到另外一个线程 如果一个变量是局部变量，那么 每个线程都会有一个该局部变量的拷贝，一个线程对该局部变量的改变不会影响到其它的线程。 多线程同步问题(重点) 为什么要引入同步机制?在多线程环境中，可能会有两个甚至更多的线程试图同时访问一个有限的资源。必须对这种潜在资源冲突进行预防。 解决方法:在线程使用一个资源时为其加锁即可。访问资源的第一个线程为其加上锁以后，其它线程便不能在使用那个资源，除非被解决。 Synchronized: 当Synchronized关键字修饰一个方法的时候，该方法叫做同步方法。java中的每个对象都有一个锁(lock)或者叫做监视器(monitor),当访问某个对象的synchronized方法的时候，表示将对象上锁，此时其它任何线程 都无法再去访问synchronized方法了，直到之前的那个线程执行方法完毕后(或者是抛出了异常)，那么将该对象的锁释放掉，其他线程才有可能再去访问该synchronized方法。 注意1: 如果一个对象有多个synchronized方法，某一个时刻某个线程已经进入到了某个synchronized方法，那么在该方法没有执行完毕前，其它线程是无法访问该对象的任何synchronzed方法的。 注意2: 如果某个Synchronized方法是static的，那么当线程访问该方法时，它锁的并不是Synchronized方法所在的对象，而是Synchronized方法所在的对象所对象的Class对象，因为java中无论一个类有多少个对象，这些对象会 对应唯一一个class对象，因此当线程分别访问同一个类的两个对象的两个static Synchronized方法的时候，他们执行的顺序也是顺序的，也就是说一个线程先去执行方法，执行完毕后另一个线程才开始执行。 写法 123synchronized(object) &#123; // 方法体&#125; synchronized方法和块比较 synchronized方法是一种粗粒度的并发控制，某一个时刻，只能有一个线程执行该synchronized方法，而synchronized块则是一种细粒度的并发控制，只会将块中的代码同步，位于方法内。synchronized块外之外的代码是可以被多个线程同时访问到的。 wait、notify、sleep方法 wait与notify方法都是定义在Object类中，而且是final的，因此会被所有的java类所继承并且无法重写，这两个方法要求在调用时线程应该已经获得了对象的锁，因此对这两个方法的调用需要方法synchronized方法或者块中，当线程执行了wait方法时，它会释放掉对象的锁。 另外一个会导致线程暂停的方法就是Thread类的sleep方法，它会导致线程睡眠指定的毫秒数，但线程在睡眠的过程中是不会释放掉对象的锁的。 Thread类中的start()和 run()方法有什么区别? start()方法被用来启动新创建的线程，而且start()内部调用了run()方法，这和直接调用run()方法的效果不一样。当你调用run()方法的时 候，只会是在原来的线程中调用，没有新的线程启动，start()方法才会启动新线程。 Java中Runnable和Callable有什么不同? Runnable和Callable都代表那些要在不同的线程中执行的任务。Runnable从JDK1.0开始就有了，Callable是在JDK1.5增加的。它们的主要区别是Callable的 call() 方法可以返回值和抛出异常，而Runnable的run()方法没有这 些功能。Callable可以返回装载有计算结果的Future对象。 Java中CyclicBarrier和 CountDownLatch有什么不同? CyclicBarrier 和 CountDownLatch 都可以用来让一组线程等待其它线程。与 CyclicBarrier 不同的是，CountdownLatch 不能重新使用。 Java中的volatile 变量是什么? volatile是一个特殊的修饰符，只有成员变量才能使用它。在Java并发程序缺少同步类的情况下，多线程对成员变量的操作对其它线程是透明的。volatile变量可以保证下一个读取操作会在前一个写操作之后发生。 什么是线程安全?Vector是一个线程安全类吗? 如果你的代码所在的进程中有多个线程在同时运行，而这些线程可能会同时运行这段代码。如果每次运行结果和单线程运行的结果是一样的，而且其他的变量的值也和预期的是一样的，就是线程安全的。一个线程 安全的计数器类的同一个实例对象在被多个线程使用的情况下也不会出现计算失误。很显然你可以将集合类分成两组，线程安全和非线程安全的。 Vector 是用同步方法来实现线程安全的, 而和它相似的ArrayList不是 线程安全的。 Java中什么是竞态条件? 举个例子说明。 竞态条件会导致程序在并发情况下出现一些bugs。多线程对一些资源的竞争的时候就会产生竞态条件，如果首先要执行的程序竞争失败排到后面执行了，那么整个程序就会出现一些不确定的bugs。这种bugs很难发现 而且会重复出现，因为线程间的随机竞争。 Java中如何停止一个线程? Java提供了很丰富的API但没有为停止线程提供API。JDK 1.0本来有一些像stop(), suspend() 和 resume()的控制方法但是由于潜在的死锁威胁因此在后续的JDK版本中他们被弃用了，之后Java API的设计者就没有提供 一个兼容且线程安全的方法来停止一个线程。当run() 或者 call() 方法执行完的时候线程会自动结束,如果要手动结束一个线程，你可以用volatile 布尔变量来退出run()方法的循环或者是取消任务来中断线程。或者使用Interrupt方法。 一个线程运行时发生异常会怎样? 如果异常没有被捕获该线程将会停止执行。Thread.UncaughtExceptionHandler是用于处理未捕获异常造成线程突然中断情况的一个内嵌接口。当一个未捕获异常将造成线程中断的时候JVM会使用Thread.getUncaughtExceptionHandler()来查询线程的UncaughtExceptionHandler并将线程和异常作为参数传递给handler的uncaughtException()方法进行处理。 如何在两个线程间共享数据? 你可以通过共享对象来实现这个目的，或者是使用像阻塞队列这样并发的数据结构。 使用Map实现线程范围内数据的共享 1234567891011121314151617181920212223242526272829303132333435363738394041public class ThreadScopeSharaData &#123; private static Map&lt;Thread, Integer&gt; threadData = new HashMap&lt;&gt;(); public static void main(String[] args) &#123; for (int i = 0; i &lt; 2; i++) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; int data = new Random().nextInt(); System.out.println(Thread.currentThread().getName() + " put random data:" + data); threadData.put(Thread.currentThread(), data); new A().get(); new B().get(); &#125; &#125;).start(); &#125; &#125; static class A &#123; public void get() &#123; int data = threadData.get(Thread.currentThread()); System.out.println("A from " + Thread.currentThread().getName() + " get data:" + data); &#125; &#125; static class B &#123; public void get() &#123; int data = threadData.get(Thread.currentThread()); System.out.println("B from " + Thread.currentThread().getName() + " get data:" + data); &#125; &#125;&#125; ThreadLocal实现线程范围内数据的共享 12345678910111213141516171819202122232425262728293031323334353637383940public class ThreadLocalTest &#123; private static ThreadLocal&lt;Integer&gt; threadLocal = new ThreadLocal&lt;&gt;(); public static void main(String[] args) &#123; for (int i = 0; i &lt; 2; i++) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; int data = new Random().nextInt(); System.out.println(Thread.currentThread().getName() + " put random data:" + data); threadLocal.set(data); new A().get(); new B().get(); &#125; &#125;).start(); &#125; &#125; static class A &#123; public void get() &#123; int data = threadLocal.get(); System.out.println("A from " + Thread.currentThread().getName() + " get data:" + data); &#125; &#125; static class B &#123; public void get() &#123; int data = threadLocal.get(); System.out.println("B from " + Thread.currentThread().getName() + " get data:" + data); &#125; &#125;&#125; Java中notify 和 notifyAll有什么区别? notify()方法不能唤醒某个具体的线程，所以只有一个线程在等待的时候它才有用武之地。而notifyAll()唤醒所有线程并允许他们争夺锁确保了至少有一个线程能继续运行。 为什么wait, notify 和 notifyAll这些方法不在thread类里面? 一个很明显的原因是JAVA提供的锁是对象级的而不是线程级的，每个对象都有锁，通过线程获得。如果线程需要等待某些锁那么调用对象中的wait()方法就有意义了。如果wait()方法定义在Thread类中，线程正 在等待的是哪个锁就不明显了。简单的说，由于wait，notify和notifyAll都是锁级别的操作，所以把他们定义在Object类中因为锁属于对象。 什么是ThreadLocal变量?ThreadLocal是Java里一种特殊的变量。 每个线程都有一个ThreadLocal就是每个线程都拥有了自己独立的一个变量，竞争条件被彻底消除了。它是为创建代价高昂的对象获取线程安全的好方法，比如你可以用ThreadLocal让SimpleDateFormat变成线程安全的，因为那个类创建代价高昂且每次调用都需要创建不同的实例所以不值得在局部范围使用它，如果为每个线程提供一个自己独有的变量拷贝，将大大提高效率。首先， 通过复用减少了代价高昂的对象的创建个数。其次，你在没有使用高代价的同步或者不变性的情况下获得了线程安全。线程局部变量的另一个不错的例子是ThreadLocalRandom类，它在多线程环境中减少了创建代价高昂的Random对象的个数。 什么是FutureTask? 在Java并发程序中FutureTask表示一个可以取消的异步运算。它有启动和取消运算、查询运算是否完成和取回运算结果等方法。只有当运算完成的时候结果才能取回，如果运算尚未完成get方法将会阻塞。一个FutureTask对象可以对调用了Callable和Runnable的对象进行包装，由于FutureTask也是调用了Runnable接口所以它可以提交给Executor来执行。 Java中interrupted和 isInterruptedd方法的区别? interrupted() 和 isInterrupted()的主要区别是前者会将中断状态清除而后者不会。Java多线程的中断机制是用内部标识来实现的，调用Thread.interrupt()来中断一个线程就会设置中断标识为true。当中断线程调用静态方 法Thread.interrupted()来检查中断状态时，中断状态会被清零。而非静态方法isInterrupted()用来查询其它线程的中断状态且不会改变中断状态标识。简单的说就是任何抛出InterruptedException异常的方法都会将中断状态清零。无论如何，一个线程的中断状态有有可能被其它线程调用中断来改变。 为什么wait和notify方法要在同步块中调用?主要是因为Java API强制要求这样做，如果你不这么做，你的代码会抛出IllegalMonitorStateException异常。还有一个原因是为了避免wait和notify之间产生竞态条件。 为什么你应该在循环中检查等待条件? 处于等待状态的线程可能会收到错误警报和伪唤醒，如果不在循环中检查等待条件，程序就会在没有满足结束条件的情况下退出。因此，当一个等待线程醒来时，不能认为它原来的等待状态仍然是有效的，在notify()方法调用之后和等待线程醒来之前这段时间它可能会改变。这就是在循环中使用wait()方法效果更好的原因。 Java中的同步集合与并发集合有什么区别? 同步集合与并发集合都为多线程和并发提供了合适的线程安全的集合，不过并发集合的可扩展性更高。在Java1.5之前程序员们只有同步集合来用且在多线程并发的时候会导致争用，阻碍了系统的扩展性。Java5介绍 了并发集合像ConcurrentHashMap，不仅提供线程安全还用锁分离和内部分区等现代技术提高了可扩展性。 Java中堆和栈有什么不同? 每个线程都有自己的栈内存，用于存储本地变量，方法参数和栈调用，一个线程中存储的变量对其它线程是不可见的。 而堆是所有线程共享的一片公用内存区域。对象都在堆里创建，为了提升效率线程会从堆中弄一个缓存到自己的栈，如果多个线程使用该变量就可能引发问题，这时volatile 变量就可以发挥作用了，它要求线程从主存中读取变量的值。 什么是线程池? 为什么要使用它? 创建线程要花费昂贵的资源和时间，如果任务来了才创建线程那么响应时间会变长，而且一个进程能创建的线程数有限。为了避免这些问题，在程序启动的时候就创建若干线程来响应处理，它们被称为线程池，里面的线程叫工作线程。从JDK1.5开始，Java API提供了Executor框架让你可以创建不同的线程池。比如单线程池，每次处理一个任务;数目固定的线程池或者是缓存线程池(一个适合很多生存期短的任务的程序的可 扩展线程池)。 如何写代码来解决生产者消费者问题? 在现实中你解决的许多线程问题都属于生产者消费者模型，就是一个线程生产任务供其它线程进行消费，你必须知道怎么进行线程间通信来解决这个问题。比较低级的办法是用wait和notify来解决这个问题，比较赞 的办法是用Semaphore 或者 BlockingQueue来实现生产者消费者模型。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import java.util.concurrent.Semaphore;import static java.lang.System.out;/** * Created by lewis on 2017/4/11. * * 生产者消费者模型 * */public class ProducerConsumerProblem&#123; //初始容量 private static final int N = 10; /*** * full 产品容量 * empty 空余容量 * mutex 读写锁 */ private static Semaphore full,empty,mutex; //记录当前的产品数量 private static volatile int count = 0 ; static &#123; /** * full 初始化0个产品 * empty 初始化有N个空余位置放置产品 * mutex 初始化每次最多只有一个线程可以读写 * */ full = new Semaphore(0); empty = new Semaphore(N); mutex = new Semaphore(1); &#125; public static void main(String []args)&#123; //生产线线程 new Thread(new Producer()).start(); //消费者线程 new Thread(new Consumer()).start(); &#125; //生产者类 static class Producer implements Runnable&#123; @Override public void run() &#123; while (true)&#123; try &#123; empty.acquire();//等待空位 mutex.acquire();//等待读写锁 count++; out.println("生产者生产了一个，还剩："+count); mutex.release();//释放读写锁 full.release();//放置产品 //随机休息一段时间，让生产者线程有机会抢占读写锁 Thread.sleep(((int)Math.random())%10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; //消费者类 static class Consumer implements Runnable&#123; @Override public void run() &#123; while (true)&#123; try &#123; full.acquire();//等待产品 mutex.acquire();//等待读写锁 count--; out.println("消费者消费了一个，还剩："+count); mutex.release();//释放读写锁 empty.release();//释放空位 //随机休息一段时间，让消费者线程有机会抢占读写锁 Thread.sleep(((int)Math.random())%10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 如何避免死锁? 死锁是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。这是一个严重的问题，因为死锁会让你的程序挂起无法完成任务，死锁的发生必 须满足以下四个条件: 互斥条件: 一个资源每次只能被一个进程使用。 请求与保持条件:一个进程因请求资源而阻塞时，对已获得的资源保持不放。 不剥夺条件:进程已获得的资源，在末使用完之前，不能强行剥夺。 循环等待条件:若干进程之间形成一种头尾相接的循环等待资源关系。 避免死锁最简单的方法就是阻止循环等待条件，将系统中所有的资源设置标志位、排序，规定所有的进程申请资源必须以一定的顺序(升序或降序)做操作来避免死锁。 Java中活锁和死锁有什么区别? 活锁和死锁类似，不同之处在于处于活锁的线程或进程的状态是不断改变的，活锁可以认为是一种特殊的饥饿。一个现实的活锁例子是两个人在狭小的走廊碰到，两个人都试着避让对方好让彼此通过，但是因为避让的方向都一样导致最后谁都不能通过走廊。简单的说就是，活锁和死锁的主要区别是前者进程的状态可以改变但是却不能继续执行。 怎么检测一个线程是否拥有锁? 在java.lang.Thread中有一个方法叫holdsLock()，它返回true如果当且仅当当前线程拥有某个具体对象的锁。 如何在Java中获取线程堆栈? 对于不同的操作系统，有多种方法来获得Java进程的线程堆栈。当你获取线程堆栈时，JVM会把所有线程的状态存到日志文件或者输出到控制台。在Windows你可以使用Ctrl + Break组合键来获取线程堆栈，Linux下用kill -3命令。你也可以用jstack这个工具来获取，它对线程id进行操作，你可以用jps这个工具找到id。 JVM中哪个参数是用来控制线程的栈堆栈小的 -Xss参数用来控制线程的堆栈大小。 Java中synchronized和 ReentrantLock有什么不同? Java在过去很长一段时间只能通过synchronized关键字来实现互斥，它有一些缺点。比如你不能扩展锁之外的方法或者块边界，尝试获取锁时不能中途取消等。Java 5通过Lock接口提供了更复杂的控制来解决这些问题。ReentrantLock 类实现了 Lock，它拥有与 synchronized 相同的并发性和内存语义且它还具有可扩展性。 有三个线程T1，T2，T3，怎么确保它们按顺序执行? 在多线程中有多种方法让线程按特定顺序执行，你可以用线程类的join()方法在一个线程中启动另一个线程，另外一个线程完成该线程继续执行。为了确保三个线程的顺序你应该先启动最后一个(T3调用T2，T2调用T1)，这样T1就会先完成而T3最后完成。 Thread类中的yield方法有什么作用? yield方法可以暂停当前正在执行的线程对象，让其它有相同优先级的线程执行。它是一个静态方法而且只保证当前线程放弃CPU占用而不能保证使其它线程一定能占用CPU，执行yield()的线程有可能在进入到暂停状态后马上又被执行。 Java中ConcurrentHashMap的并发度是什么? ConcurrentHashMap把实际map划分成若干部分来实现它的可扩展性和线程安全。这种划分是使用并发度获得的，它是ConcurrentHashMap类构造函数的一个可选参数，默认值为16，这样在多线程情况下就能避免争用。 Java中Semaphore是什么? Java中的Semaphore是一种新的同步类，它是一个计数信号。从概念上讲，从概念上讲，信号量维护了一个许可集合。如有必要，在许可可用前会阻塞每一个 acquire()，然后再获取该许可。每个 release()添加一个许可，从而可能释放一个正在阻塞的获取者。但是，不使用实际的许可对象，Semaphore只对可用许可的号码进行计数，并采取相应的行动。信号量常常用于多线程的代码中，比如数据库连接池。 如果你提交任务时，线程池队列已满。会发生什么? 事实上如果一个任务不能被调度执行那么ThreadPoolExecutor’s submit()方法将会抛出一个RejectedExecutionException异常 Java线程池中submit() 和 execute()方法有什么区别? 两个方法都可以向线程池提交任务，execute()方法的返回类型是void，它定义在Executor接口中, 而submit()方法可以返回持有计算结果的Future对象，它定义在ExecutorService接口中，它扩展了Executor接口，其它线程池类像ThreadPoolExecutor和ScheduledThreadPoolExecutor都有这些方法。 什么是阻塞式方法? 阻塞式方法是指程序会一直等待该方法完成期间不做其他事情，ServerSocket的accept()方法就是一直等待客户端连接。这里的阻塞是指调用结果返回之前，当前线程会被挂起，直到得到结果之后才会返回。此外，还 有异步和非阻塞式方法在任务完成前就返回。 Java中invokeAndWait 和 invokeLater有什么区别? 这两个方法是Swing API 提供给Java开发者用来从当前线程而不是事件派发线程更新GUI组件用的。InvokeAndWait()同步更新GUI组件，比如一个进度条，一旦进度更新了，进度条也要做出相应改变。如果进度被多 个线程跟踪，那么就调用invokeAndWait()方法请求事件派发线程对组件进行相应更新。而invokeLater()方法是异步调用更新组件的。 如何在Java中创建Immutable对象? 这个问题看起来和多线程没什么关系， 但不变性有助于简化已经很复杂的并发程序。Immutable对象可以在没有同步的情况下共享，降低了对该对象进行并发访问时的同步化开销。可是Java没有@Immutable这个注解符，要创建不可变类，要实现下面几个步骤:通过构造方法初始化所有成员、对变量不要提供setter方法、将所有的成员声明为私有的，这样就不允许直接访问这些成员、在getter方法中，不要直接返回对象本身，而是克隆对象，并返回对象的拷贝。 Java中的ReadWriteLock是什么? 一般而言，读写锁是用来提升并发程序性能的锁分离技术的成果。Java中的ReadWriteLock是Java 5 中新增的一个接口，一个ReadWriteLock维护一对关联的锁，一个用于只读操作一个用于写。在没有写线程的情况下 一个读锁可能会同时被多个读线程持有。写锁是独占的，你可以使用JDK中的ReentrantReadWriteLock来实现这个规则，它最多支持65535个写锁和65535个读锁。 多线程中的忙循环是什么? 忙循环就是程序员用循环让一个线程等待，不像传统方法wait(), sleep() 或 yield() 它们都放弃了CPU控制，而忙循环不会放弃CPU，它就是在运行一个空循环。这么做的目的是为了保留CPU缓存，在多核系统中，一个 等待线程醒来的时候可能会在另一个内核运行，这样会重建缓存。为了避免重建缓存和减少等待重建的时间就可以使用它了。 volatile 变量和 atomic 变量有什么不同? 这是个有趣的问题。首先，volatile 变量和 atomic 变量看起来很像，但功能却不一样。volatile变量可以确保先行关系，即写操作会发生在后续的读操作之前, 但它并不能保证原子性。例如用volatile修饰count变量那么count++ 操作就不是原子性的。而AtomicInteger类提供的atomic方法可以让这种操作具有原子性如getAndIncrement()方法会原子性的进行增量操作把当前值加一，其它数据类型和引用变量也可以进行相似操作。 如果同步块内的线程抛出异常会发生什么? 无论你的同步块是正常还是异常退出的，里面的线程都会释放锁，所以对比锁接口我更喜欢同步块，因为它不用我花费精力去释放锁，该功能可以在finally block里释放锁实现。 写出3条你遵循的多线程最佳实践 给你的线程起个有意义的名字。 这样可以方便找bug或追踪。OrderProcessor, QuoteProcessor or TradeProcessor 这种名字比 Thread-1. Thread-2 and Thread-3 好多了，给线程起一个和它要完成的任务相关的名字，所有的主要框架甚至JDK都遵循这个 最佳实践。 避免锁定和缩小同步的范围 锁花费的代价高昂且上下文切换更耗费时间空间，试试最低限度的使用同步和锁，缩小临界区。因此相对于同步方法我更喜欢同步块，它给我拥有对锁的绝对控制权。 多用同步类少用wait 和 notify 首先，CountDownLatch, Semaphore, CyclicBarrier 和 Exchanger 这些同步类简化了编码操作，而用wait和notify很难实现对复杂控制流的控制。其次，这些类是由最好的企业编写和维护在后续的JDK中它们还会不断优化和完善，使用这些更高等级的同步工具你的程序可以不费吹灰之力获得优化。 多用并发集合少用同步集合 这是另外一个容易遵循且受益巨大的最佳实践，并发集合比同步集合的可扩展性更好，所以在并发编程时使用并发集合效果更好。如果下一次你需要用到map，你应该首先想到用ConcurrentHashMap。 如何强制启动一个线程? 这个问题就像是如何强制进行Java垃圾回收，目前还没有觉得方法，虽然你可以使用System.gc()来进行垃圾回收，但是不保证能成功。在Java里面没有办法强制启动一个线程，它是被线程调度器控制着且Java没有公布相关的API Java中的fork join框架是什么? fork join框架是JDK7中出现的一款高效的工具，Java开发人员可以通过它充分利用现代服务器上的多处理器。它是专门为了那些可以递归划分成许多子模块设计的，目的是将所有可用的处理能力用来提升程序的性能。fork join框架一个巨大的优势是它使用了工作窃取算法，可以完成更多任务的工作线程可以从其它线程中窃取任务来执行。]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用git进行项目版本管理]]></title>
    <url>%2F2019%2F07%2F09%2Flearn-git%2F</url>
    <content type="text"><![CDATA[本文以Apache Kylin为例，介绍使用git进行项目分支管理，以及常见开发中的问题及解决方法。 常见的分支类型 上图为截取的最近Apache Kylin项目的分支图，目前主要有以下几种分支类型： master分支 维护分支 feature分支 release分支 作用 最新的代码，随时会被更新 维护版本的代码，理论上只接受改动较小的enhancement和bug fix 为开发一些大改动的功能单拉的分支 用作版本发布的分支 例子 master 2.6.x engine-flink, kylin-on-druid v2.6.3-release, v-3.0.0-alpha-release 生命周期 从项目诞生到删除 从一个稳定的维护系列版本产生到该系列版本停止维护 从feature诞生到合并进入主分支 从准备发布到发布正式annunce 每一个版本release后，都会打上对应的tag，目前有kylin-2.6.3, kylin-3.0.0-alpha等 常用的git命令基本操作 查看 1234567891011121314151617git status #显示有变更的文件git log #显示当前分支的版本历史git log --stat #显示commit历史，以及每次commit发生变更的文件git blame [file] #显示指定文件是什么人在什么时间修改过 git diff #显示暂存区和工作区的差异git diff HEAD #显示工作区与当前分支最新commit之间的差异git show [commit] #显示某次提交的元数据和内容变化git show [commit]:[filename] #显示某次提交时，某个文件的内容git reflog #显示当前分支的最近几次提交 增加/删除文件 1234567891011git add [file1] #添加指定文件到暂存区git add [dir] #添加指定目录到暂存区，包括子目录git add . #添加当前目录的所有文件到暂存区git rm [file1] #删除工作区文件，并且将这次删除放入暂存区git rm --cached #[file] 停止追踪指定文件，但该文件会保留在工作区git mv &lt;file_original&gt; &lt;file_renamed&gt; #改名文件，并且将这个改名放入暂存区 代码提交 1234567891011git commit -m [message] #提交暂存区到仓库区git commit [file1] -m [message] #提交暂存区的指定文件到仓库区git commit -a #提交工作区自上次commit之后的变化，直接到仓库区git commit -v #提交时显示所有diff信息git commit --amend -m [message] #使用一次新的commit，替代上一次提交. 如果代码没有任何新变化，则用来改写上一次commit的提交信息git commit --amend [file1] #重做上一次commit，并包括指定文件的新变化 远程仓库 12345678910111213141516171819202122git clone [远程仓库地址] #从远程仓库克隆git fetch &lt;远程主机名&gt; &lt;分支名&gt; #更新远程仓库的所有变动，如git fetch origin master# 要更新所有分支，命令可以简写为 git fetchgit remote -v #显示所有远程仓库git remote show [remote] #显示某个远程仓库的信息git remote add [shortname] [url] #增加一个新的远程仓库，并命名git pull [remote] [branch] # 取回远程仓库的变化，并与本地分支合并git push &lt;远程主机名&gt; &lt;本地分支名&gt;:&lt;远程分支名&gt; #将本地分支的更新，推送到远程主机# 如果省略远程分支名，则表示将本地分支推送与之存在"追踪关系"的远程分支 git push origin master # 如果当前分支与远程分支之间存在追踪关系, 则本地分支和远程分支都可以省略 git push origin# 如果当前分支只有一个追踪分支，那么主机名都可以省略 git push# 如果当前分支与多个主机存在追踪关系，则可以使用-u选项指定一个默认主机，这样后面就可以不加任何参数使用git push# ！！！！慎用，有被杀了祭天的风险 ！！！！git push --force origin # 如果远程主机的版本比本地版本更新，推送时Git会报错，要求先在本地做git pull合并差异，然后再推送到远程主机。这时，如果你一定要推送，可以使用--force选项# ！！！！慎用，有被杀了祭天的风险 ！！！！ 标签管理 1234567891011121314151617git tag &lt;name&gt; #在当前分支打标签git tag #查看所有标签git show &lt;tagname&gt; #查看标签信息git tag -a &lt;tagname&gt; -m "" #可以指定标签信息git push origin &lt;tagname&gt; #推送一个本地标签git push origin --tags #推送全部未推送过的本地标签git tag -d &lt;tagname&gt; #删除一个本地标签git push origin:refs/tags/&lt;tagname&gt; #删除一个远程标签git checkout &lt;tagname&gt; #checkout到一个指定tag下对应的代码 分支管理 12345678910111213git checkout -b dev #创建并切换到dev分支git branch dev #创建分支git checkout dev #切换分支git branch #查看当前分支git merge dev #当前分支与dev分支合并git branch -d dev #删除dev分支 git push --set-upstream &lt;remote&gt; &lt;branch_name&gt; #将当前分支提交到远端 撤销 1234567891011121314151617181920git checkout [file] #恢复暂存区的指定文件到工作区git checkout [commit] [file] #恢复某个commit的指定文件到暂存区和工作区git checkout . #恢复暂存区的所有文件到工作区git reset [file] #重置暂存区的指定文件，与上一次commit保持一致，但工作区不变git reset --hard #重置暂存区与工作区，与上一次commit保持一致 git reset [commit] #重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变git reset --hard [commit] #重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致git reset --keep [commit] #重置当前HEAD为指定commit，但保持暂存区和工作区不变git revert [commit] #新建一个commit，用来撤销指定commit 后者的所有变化都将被前者抵消，并且应用到当前分支git stash #暂时将未提交的变化移除git stash pop #将在缓冲区的变动移入 理解HEADGit 中的 HEAD 可以理解为指针，指向当前仓库所处的分支。一般在有 git 管理的目录下打开 git 终端都能在当前路径的尾巴上，看到所处的分支名。 当使用 git checkout &lt; branch_name&gt; 切换分支时，HEAD 会移动到指定分支。 但是如果使用的是 git checkout &lt;commit id&gt;，即切换到指定的某一次提交，HEAD 就会处于 detached 状态（游离状态）。在这个基础上的提交会新开一个匿名分支。 常见案例如何提交一个Kylin的Pull Request Fork Apache Kylin项目，则会生成个人的Kylin项目，如https://github.com/nichunen/kylin.git Clone个人fork的项目到本地 git clone https://github.com/nichunen/kylin.git 基于master分支或feature分支新建一个分支，一般为Jira issue号 git checkout -b KYLIN-4042 基于上述分支进行开发 将改动commit到本地，并push到远程 在github打开apache/kylin项目，页面会自动提示创建PR 正确地进行rebaseRebase 实际上就是取出一系列的提交记录，“复制”它们，然后在另外一个地方逐个的放下去。Rebase 的优势就是可以创造更线性的提交历史。如果只允许使用 Rebase 的话，代码库的提交历史将会变得异常清晰。 如下图的项目，现在我们需要新建并切换到 bugFix 分支，进行提交一次，然后切换回 master 分支再提交一次。再次切换到 bugFix 分支，rebase master 上的改动。 命令大致如下 123456git checkout -b BugFixgit commitgit checkout mastergit commitgit checkout BugFixgit rebase master 如何快速向上移动一个/多个commit12git checkout master^git checkout master~&#123;number&#125; 如何进行cherry-pick如下图的分支结构，现在需要将三个分支中的C3,C4,C7提交记录复制到 master 上 git cherry-pick C3 C4 C7 使用rebase -i对commit进行合并与调整如下图项目的commit记录 现在需要将最上面的两个commit合并为一个commit，这里就用到利器rebase -i命令 1git rebase -i daee8c23fc740fc99a5be6a7fbc6dc8c78e3a46e 出现如下编辑页面 将第二个commit修改为s(squash)，这个commit会被合并至前一个commit 出现如下编辑页面，修改commit message如下 再执行git log，最近的commit即变更为一个合并的commit 执行git push -f强行提交 生成和合并patch将最上面的2个commit生成为2个patch文件 1git format-patch HEAD^^ 执行patch文件的代码修改 1git apply 0001-This-is-commit-1-2.patch 执行patch文件的代码修改并commit，并忽略空格 1git am -3 --ignore-whitespace 0001-This-is-commit-1-2.patch]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kylin的任务调度介绍]]></title>
    <url>%2F2019%2F03%2F30%2FKylin-s-job-scheduler%2F</url>
    <content type="text"><![CDATA[Kylin的任务调度介绍Kylin的任务调度模式采用的是生产者-消费者模式。 Kylin的构建、合并、优化segment任务的入口为EngineFactory的createBatchCubingJob, createBatchMergeJob, createBatchOptimizeJob方法。EngineFactory是一个工厂类，可以根据不同的配置选择不同的cube构建类型（Batch或者Streaming）和不同的cube构建算法（Batch Cube有不同的构建算法）。 Kylin的可执行任务都被表示为一个Executable对象，基本的UML图如下 Executable的核心方法是execute，每个实现类可以定义Job的具体执行逻辑。 1234567891011121314151617public interface Executable &#123; String getId(); String getName(); //定义Job的具体执行逻辑 //参数ExecutableContext Job的上下文 //结果ExecuteResult Job的状态和输出 ExecuteResult execute(ExecutableContext executableContext) throws ExecuteException; //获取Job运行状态 ExecutableState getStatus(); Output getOutput(); boolean isRunnable(); //获取Job的执行参数 Map&lt;String, String&gt; getParams();&#125; 抽象类AbstractExecutable实现了Executable接口，核心是实现了execute方法，为了清晰的定义每个Job的运行状态，AbstractExecutable将execute方法细化为onExecuteStart，doWork，onExecuteError，onExecuteFinished等阶段。其中execute方法修饰符为final，onExecuteStart，onExecuteError，onExecuteFinished方法修饰符为protected，doWork方法修饰符为protected abstract，用于子类根据自己的具体逻辑重写此方法。 execute方法的关键代码如下： 1234567891011121314public final ExecuteResult execute(ExecutableContext executableContext) throws ExecuteException &#123; //Job的状态从Ready 变为 Running onExecuteStart(executableContext); ... //不同的Job在这里实现具体逻辑 result = doWork(executableContext); ... if (exception != null) &#123; //Job的状态从Ready 变为 Error onExecuteError(exception, executableContext); &#125; //Job的状态从Ready 变为 Succeed or Error or Discard onExecuteFinished(result, executableContext); &#125; AbstractExecutable的具体直接实现类主要有ShellExecutable，HadoopShellExecutable，MapReduceExecutable，主要是根据自身的具体逻辑重写了doWork`方法。 ShellExecutable主要用来执行Shell 命令可以直接执行的Job，像计算Hive表行数等Job。 HadoopShellExecutable主要用来执行依赖Hadoop环境且用Shell执行的Job，像建字典，建立HBase表，Bulkload HFile等Job。 MapReduceExecutable主要用来执行MapReduce类型的Job，像计算列基数，计算Cuboid， 生成HFile等Job。 前面提到是CubingJob将构建cube的每一步job串了起来，其实CubingJob继承了DefaultChainedExecutable，DefaultChainedExecutable类继承了 AbstractExecutable类并实现了ChainedExecutable接口。CubingJob 主要是为Job关联了cube和segment的相关信息，串连所有Job的任务都是DefaultChainedExecutable类实现的。 123456public interface ChainedExecutable extends Executable &#123; //获取所有子Job List&lt;? extends AbstractExecutable&gt; getTasks(); //添加子Job void addTask(AbstractExecutable executable);&#125; 之前提到在生成Job的时候，CubingJob通过addTask方法将所有子Job串连了起来。那么DefaultChainedExecutable到底是如何串连起所有子Job呢？关键在其重写的doWork方法里： 123456789101112131415161718protected ExecuteResult doWork(ExecutableContext context) throws ExecuteException &#123; List&lt;? extends Executable&gt; executables = getTasks();//获取所有子Job for (int i = 0; i &lt; executables.size(); ++i) &#123; Executable subTask = executables.get(i); ExecutableState state = subTask.getStatus(); if (state == ExecutableState.RUNNING) &#123; //子Job正在执行，等待它完成 break; &#125; else if (state == ExecutableState.ERROR) &#123; //子Job执行失败，抛出异常 &#125; if (subTask.isRunnable()) &#123; //每个Job在初始化后是Ready状态，所以isRunnable()是True，当子Job是Ready状态时，就开始执行。 return subTask.execute(context); &#125; &#125; return new ExecuteResult(ExecuteResult.State.SUCCEED, null); &#125; DefaultChainedExecutable 也重写了onExecuteFinished方法，来根据所有子Job的状态更新整个Job的最终状态。 Kylin默认通过DefaultScheduler进行任务的调度，其核心逻辑是十分简单的，有两个线程池，一个线程池用来抓取所有Job的状态信息，一个线程池来执行具体的Job。基本的逻辑过程如下： 参考：https://blog.bcmeng.com/post/kylin-job.html Category KYLIN issue Description KAP issue Status Comments Bug 4107 Fix bug of StorageCleanupJob fails to delete Hive tables with “Argument list too long” error P2 表名过长清理时报错，建议引入 Enhancement 4117 Auto adjust data type of RelNode for intersect_count P2 Rest客户端有可能抛出带有明文Klin密码的异常信息。需排查KE是否有此问题，若有，建议引入 Enhancement 4137 Accelerate metadata reloading P2 优化元数据重载，建议引入 Enhancement 4101 Set hive and spark job name when building cube P3 写起在yarn上的hive及spark任务的任务名，方便检查。无风险，建议引入 Enhancement 4095 Add RESOURCE_PATH_PREFIX option in ResourceTool P3 Resume人物后，任务输出没有清空，留的是上一次跑的输出结果，建议引入 Enhancement 4092 Support setting seperate jvm params for kylin backgroud tools P2 支持为使用kylin.sh跑的脚本工具配置jvm参数 Enhancement 4091 Support fast mode and simple mode for running CI P2 支持CI中cube的并发构建，来缩短CI时间，建议引入]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kylin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kylin的元数据-任务]]></title>
    <url>%2F2019%2F03%2F30%2FKylin-s-metadata-execute%2F</url>
    <content type="text"><![CDATA[Kyin任务元数据基本介绍在Kylin的元数据基础一文中介绍了kylin元数据的一些源码级基础知识。使用 Kylin生成每执行一个构建或合并任务，都会在Monitor页面生成一个job Kylin使用存于resource store的metadata来实现元数据的持久化保存。 对于任务元数，Kylin使用了2中类型的元数据，分别为execute和execute_output execute：存储与表示任务的执行计划 execute_output：存储与表示任务的执行结果 每个任务仅有一个execute元数据，会有(step_num + 1)个execute_output元数据，如下图所示为一dump下来的某个任务的元数据 execute的基本元数据类为org.apache.kylin.job.dao.ExecutablePO，代码为 1234567891011121314151617181920public class ExecutablePO extends RootPersistentEntity &#123; // 任务名 @JsonProperty("name") private String name; // 子任务列表 @JsonProperty("tasks") private List&lt;ExecutablePO&gt; tasks; // 任务类型 @JsonProperty("type") private String type; // 任务参数 @JsonProperty("params") private Map&lt;String, String&gt; params = Maps.newHashMap(); //....&#125; 生成的json元数据文件示例如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&#123; "uuid": "99b80122-b59b-bbdb-5f22-26d1d9270220", "last_modified": 1555246404591, "version": "2.6.0.20500", "name": "BUILD CUBE - kylin_sales_cube - 20120101000000_20190430185000 - GMT+08:00 2019-04-14 20:53:24", "tasks": [ &#123; "uuid": "99b80122-b59b-bbdb-5f22-26d1d9270220-00", "last_modified": 0, "version": "2.6.0.20500", "name": "Create Intermediate Flat Hive Table", "tasks": null, "type": "org.apache.kylin.source.hive.CreateFlatHiveTableStep", "params": &#123; "HiveInit": "USE default; ", "HiveRedistributeData": "DROP TABLE IF EXISTS kylin_intermediate_kylin_sales_cube_86f98364_1485_001b_8fc5_18ec50930a14; CREATE EXTERNAL TABLE IF NOT EXISTS kylin_intermediate_kylin_sales_cube_86f98364_1485_001b_8fc5_18ec50930a14 ( KYLIN_SALES_TRANS_ID bigint ,KYLIN_SALES_PART_DT date ,KYLIN_SALES_LEAF_CATEG_ID bigint ,KYLIN_SALES_LSTG_SITE_ID int ,KYLIN_CATEGORY_GROUPINGS_META_CATEG_NAME string ,KYLIN_CATEGORY_GROUPINGS_CATEG_LVL2_NAME string ,KYLIN_CATEGORY_GROUPINGS_CATEG_LVL3_NAME string ,KYLIN_SALES_LSTG_FORMAT_NAME string ,KYLIN_SALES_SELLER_ID bigint ,KYLIN_SALES_BUYER_ID bigint ,BUYER_ACCOUNT_ACCOUNT_BUYER_LEVEL int ,SELLER_ACCOUNT_ACCOUNT_SELLER_LEVEL int ,BUYER_ACCOUNT_ACCOUNT_COUNTRY string ,SELLER_ACCOUNT_ACCOUNT_COUNTRY string ,BUYER_COUNTRY_NAME string ,SELLER_COUNTRY_NAME string ,KYLIN_SALES_OPS_USER_ID string ,KYLIN_SALES_OPS_REGION string ,KYLIN_SALES_PRICE decimal(19,4) ) STORED AS SEQUENCEFILE LOCATION 'hdfs://sandbox.hortonworks.com:8020/Users/nichunen/Downloads/template/kylin-99b80122-b59b-bbdb-5f22-26d1d9270220/kylin_intermediate_kylin_sales_cube_86f98364_1485_001b_8fc5_18ec50930a14'; ALTER TABLE kylin_intermediate_kylin_sales_cube_86f98364_1485_001b_8fc5_18ec50930a14 SET TBLPROPERTIES('auto.purge'='true'); INSERT OVERWRITE TABLE `kylin_intermediate_kylin_sales_cube_86f98364_1485_001b_8fc5_18ec50930a14` SELECT `KYLIN_SALES`.`TRANS_ID` as `KYLIN_SALES_TRANS_ID` ,`KYLIN_SALES`.`PART_DT` as `KYLIN_SALES_PART_DT` ,`KYLIN_SALES`.`LEAF_CATEG_ID` as `KYLIN_SALES_LEAF_CATEG_ID` ,`KYLIN_SALES`.`LSTG_SITE_ID` as `KYLIN_SALES_LSTG_SITE_ID` ,`KYLIN_CATEGORY_GROUPINGS`.`META_CATEG_NAME` as `KYLIN_CATEGORY_GROUPINGS_META_CATEG_NAME` ,`KYLIN_CATEGORY_GROUPINGS`.`CATEG_LVL2_NAME` as `KYLIN_CATEGORY_GROUPINGS_CATEG_LVL2_NAME` ,`KYLIN_CATEGORY_GROUPINGS`.`CATEG_LVL3_NAME` as `KYLIN_CATEGORY_GROUPINGS_CATEG_LVL3_NAME` ,`KYLIN_SALES`.`LSTG_FORMAT_NAME` as `KYLIN_SALES_LSTG_FORMAT_NAME` ,`KYLIN_SALES`.`SELLER_ID` as `KYLIN_SALES_SELLER_ID` ,`KYLIN_SALES`.`BUYER_ID` as `KYLIN_SALES_BUYER_ID` ,`BUYER_ACCOUNT`.`ACCOUNT_BUYER_LEVEL` as `BUYER_ACCOUNT_ACCOUNT_BUYER_LEVEL` ,`SELLER_ACCOUNT`.`ACCOUNT_SELLER_LEVEL` as `SELLER_ACCOUNT_ACCOUNT_SELLER_LEVEL` ,`BUYER_ACCOUNT`.`ACCOUNT_COUNTRY` as `BUYER_ACCOUNT_ACCOUNT_COUNTRY` ,`SELLER_ACCOUNT`.`ACCOUNT_COUNTRY` as `SELLER_ACCOUNT_ACCOUNT_COUNTRY` ,`BUYER_COUNTRY`.`NAME` as `BUYER_COUNTRY_NAME` ,`SELLER_COUNTRY`.`NAME` as `SELLER_COUNTRY_NAME` ,`KYLIN_SALES`.`OPS_USER_ID` as `KYLIN_SALES_OPS_USER_ID` ,`KYLIN_SALES`.`OPS_REGION` as `KYLIN_SALES_OPS_REGION` ,`KYLIN_SALES`.`PRICE` as `KYLIN_SALES_PRICE` FROM `DEFAULT`.`KYLIN_SALES` as `KYLIN_SALES` INNER JOIN `DEFAULT`.`KYLIN_CAL_DT` as `KYLIN_CAL_DT` ON `KYLIN_SALES`.`PART_DT` = `KYLIN_CAL_DT`.`CAL_DT` INNER JOIN `DEFAULT`.`KYLIN_CATEGORY_GROUPINGS` as `KYLIN_CATEGORY_GROUPINGS` ON `KYLIN_SALES`.`LEAF_CATEG_ID` = `KYLIN_CATEGORY_GROUPINGS`.`LEAF_CATEG_ID` AND `KYLIN_SALES`.`LSTG_SITE_ID` = `KYLIN_CATEGORY_GROUPINGS`.`SITE_ID` INNER JOIN `DEFAULT`.`KYLIN_ACCOUNT` as `BUYER_ACCOUNT` ON `KYLIN_SALES`.`BUYER_ID` = `BUYER_ACCOUNT`.`ACCOUNT_ID` INNER JOIN `DEFAULT`.`KYLIN_ACCOUNT` as `SELLER_ACCOUNT` ON `KYLIN_SALES`.`SELLER_ID` = `SELLER_ACCOUNT`.`ACCOUNT_ID` INNER JOIN `DEFAULT`.`KYLIN_COUNTRY` as `BUYER_COUNTRY` ON `BUYER_ACCOUNT`.`ACCOUNT_COUNTRY` = `BUYER_COUNTRY`.`COUNTRY` INNER JOIN `DEFAULT`.`KYLIN_COUNTRY` as `SELLER_COUNTRY` ON `SELLER_ACCOUNT`.`ACCOUNT_COUNTRY` = `SELLER_COUNTRY`.`COUNTRY` WHERE 1=1 AND (`KYLIN_SALES`.`PART_DT` &gt;= '2012-01-01' AND `KYLIN_SALES`.`PART_DT` &lt; '2019-04-30') ; ", "cubeName": "kylin_sales_cube" &#125; &#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125; ], "type": "org.apache.kylin.engine.mr.CubingJob", "params": &#123; "submitter": "ADMIN", "envName": "DEV", "segmentId": "86f98364-1485-001b-8fc5-18ec50930a14", "notify_list": "", "projectName": "learn_kylin", "jobType": "BUILD", "cubeName": "kylin_sales_cube", "segmentName": "20120101000000_20190430185000" &#125;&#125; execute_output的基本元数据类为org.apache.kylin.job.dao.ExecutableOutputPO，代码为 1234567891011public class ExecutableOutputPO extends RootPersistentEntity &#123; @JsonProperty("content") private String content; @JsonProperty("status") private String status = "READY"; @JsonProperty("info") private Map&lt;String, String&gt; info = Maps.newHashMap();&#125; 生成的全局任务json元数据文件示例如下： 1234567891011121314&#123; "uuid": "99b80122-b59b-bbdb-5f22-26d1d9270220", "last_modified": 1555246411812, "version": "2.6.0.20500", "content": "org.apache.kylin.job.exception.ExecuteException: java.io.IOException: OS command error exit with return code: 127, error message: /bin/bash: hive: command not foundThe command is: hive -e \"", "status": "ERROR", "info": &#123; "startTime": "1555246411762", "buildInstance": "62641@GggdeMacBook-Pro.local", "endTime": "1555246411811" &#125;&#125; 生成的子任务json元数据文件示例如下： 12345678910111213&#123; "uuid": "99b80122-b59b-bbdb-5f22-26d1d9270220-00", "last_modified": 1555246411809, "version": "2.6.0.20500", "content": "java.io.IOException: OS command error exit with return code: 127, error message: /bin/bash: hive: command not foundThe command is: hive -e \"", "status": "ERROR", "info": &#123; "startTime": "1555246411769", "endTime": "1555246411807" &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kylin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kylin的元数据基础]]></title>
    <url>%2F2019%2F03%2F30%2FKylin-s-metadata-base%2F</url>
    <content type="text"><![CDATA[Apache Kylin元数据基础Apache Kylin的元数据包括 立方体描述（cube description）、立方体实例（cube instance）、项目（project）、模型描述（model description）、作业（job）、表（table）、字典(dictionary)等，参见： Kylin核心概念。在kylin集群中至关重要，假如元数据丢失，kylin集群将无法工作。 Kylin源码定义了ResourceStore这一抽象类来定义，其中定义了各种对元数据增删改查的方法，主要方法如： 1234567891011121314151617// 根据kylinConfig获得对应的ResourceStoreResourceStore getStore(KylinConfig kylinConfig)// 根据目录获取所有元数据路径NavigableSet&lt;String&gt; listResources(String folderPath)// 路径resPath下是否存在元数据boolean exists(String resPath) // 根据路径resPath及序列化器serializer返回对应的元数据&lt;T extends RootPersistentEntity&gt; T getResource(String resPath, Serializer&lt;T&gt; serializer) // 根据路径resPath写入元数据 public &lt;T extends RootPersistentEntity&gt; long putResource(String resPath, T obj, long ts, Serializer&lt;T&gt; serializer) // 删除路径resPath下的元数据void deleteResource(String resPath) ResourceStore类的主要子类继承关系图如下： Kylin支持多种数据源格式，如Hbase、Jdbc等，只要继承ResourceStore类并实现必须的Impl方法即可。 Kylin的所有序列化的元数据都继承自RootPersistentEntity抽象类，该类及其子类都使用了jackson的JsonAutoDetect注解。 例如CubeDesc部分代码如下： 1234567891011121314151617181920212223242526272829@SuppressWarnings(&quot;serial&quot;)@JsonAutoDetect(fieldVisibility = Visibility.NONE, getterVisibility = Visibility.NONE, isGetterVisibility = Visibility.NONE, setterVisibility = Visibility.NONE)public class CubeDesc extends RootPersistentEntity implements IEngineAware &#123; //... @JsonProperty(&quot;name&quot;) private String name; @JsonProperty(&quot;is_draft&quot;) private boolean isDraft; @JsonProperty(&quot;model_name&quot;) private String modelName; @JsonProperty(&quot;description&quot;) private String description; @JsonProperty(&quot;null_string&quot;) private String[] nullStrings; @JsonProperty(&quot;dimensions&quot;) private List&lt;DimensionDesc&gt; dimensions; @JsonProperty(&quot;measures&quot;) private List&lt;MeasureDesc&gt; measures; @JsonProperty(&quot;dictionaries&quot;) @JsonInclude(JsonInclude.Include.NON_NULL) private List&lt;DictionaryDesc&gt; dictionaries; @JsonProperty(&quot;rowkey&quot;) private RowKeyDesc rowkey; @JsonProperty(&quot;hbase_mapping&quot;) private HBaseMappingDesc hbaseMapping; @JsonProperty(&quot;aggregation_groups&quot;) private List&lt;AggregationGroup&gt; aggregationGroups; // ...&#125; 对应在元数据库的json数据如下： 12345678910111213141516&#123; "uuid":"0ef9b7a8-3929-4dff-b59d-2100aadc8dbf", "last_modified":1451468470824, "version":"%default_version%", "name":"kylin_sales_cube", "is_draft":false, "model_name":"kylin_sales_model", "description":"", "null_string":null, "dimensions":[], "measures":[], "rowkey":&#123;&#125;, "hbase_mapping":&#123;&#125;, "aggregation_groups":[], ...&#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kylin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OSI七层模型的通俗理解]]></title>
    <url>%2F2019%2F03%2F12%2Fosi-tcpip%2F</url>
    <content type="text"><![CDATA[OSI七层模型的通俗理解 以你和你女朋友以书信的方式进行通信为例。 物理层：运输工具，比如火车、汽车 数据链路层：相当于货物核对单，表明里面有些什么东西，接受的时候确认一下是否正确（CRC检验） 网络层：相当于邮政局或快递公司地址（IP地址），能正确到达对方 传输层：信封（TCP协议是挂号信，是可靠的；UDP协议是平信，尽力送到对方，不保证一点送到对方） 会话层：相当于邮票，优质邮票寄一封信，相当与一个会话 表示层：你用普通话还是用方言？或者是英语？ 应用层：你可以说你的内容了，可以说是你爱她，也可以说你恨她。]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用gogs安装本地git服务(Centos环境)]]></title>
    <url>%2F2019%2F03%2F11%2Fgogs-install%2F</url>
    <content type="text"><![CDATA[使用gogs安装本地git服务(Centos环境)安装数据库Gogs支持MySQL、PostgreSQL、SQLite3、TiDB。安装过程这里不赘述，我们使用的是MySQL。 安装git1yum install -y git 添加git用户（gogs期望用git用户操作）1sudo useradd git 下载并安装根据自己的linux版本在 https://dl.gogs.io/ 下载安装包，linux版本根据uname -a查看 1wget https://dl.gogs.io/0.11.86/gogs_0.11.86_linux_amd64.tar.gz 解压 1tar -zxvf gogs_0.11.86_linux_amd64.tar.gz 进入解压的目录，执行 12cd gogs/./gogs web 打开${ip}:3000页面，出现如下配置页面 配置完数据库、端口、用户信息等 运行gogsControl+C暂停当前gogs进程，后台运行gogs 1nohup ./gogs web &amp; 打开${ip}:3000页面，注册后登陆]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive 常用命令]]></title>
    <url>%2F2019%2F03%2F10%2Fhive-cli%2F</url>
    <content type="text"><![CDATA[Hive常用命令创建表： 1CREATE TABLE pokes (foo INT, bar STRING); 创建一个新表，结构与其他一样： 1create table new_table like records; 创建分区表： 1create table logs(ts bigint,line string) partitioned by (dt String,country String); 加载分区表数据： 1load data local inpath '/home/Hadoop/input/hive/partitions/file1' into table logs partition (dt='2001-01-01',country='GB'); 展示表中有多少分区： 1show partitions logs; 展示所有表： 1SHOW TABLES; 显示表的结构信息： 1DESCRIBE invites; 更新表的名称： 1ALTER TABLE source RENAME TO target; 添加新一列： 1ALTER TABLE invites ADD COLUMNS (new_col2 INT COMMENT 'a comment'); 添加新的一行： 1INSERT INTO test(name,pwd,createdate) values('name1','pwd1','2017-06-20 14:14:09'); 删除表： 1DROP TABLE records; 删除表中数据，但要保持表的结构定义： 1TRUNCATE TABLE table_name 从本地文件加载数据： 1LOAD DATA LOCAL INPATH '/home/hadoop/input/ncdc/micro-tab/sample.txt' OVERWRITE INTO TABLE records; 显示所有函数： 1show functions; 查看函数用法： 1describe function substr; 内连接： 1SELECT sales., things. FROM sales JOIN things ON (sales.id = things.id); 查看hive为某个查询使用多少个MapReduce作业: 1Explain SELECT sales., things. FROM sales JOIN things ON (sales.id = things.id); 外连接： 1SELECT sales., things. FROM sales LEFT OUTER JOIN things ON (sales.id = things.id); 创建视图： 1CREATE VIEW valid_records AS SELECT * FROM records2 WHERE temperature !=9999; INSERT OVERWRITE： 1INSERT OVERWRITE table tablename1 select a, b, c from tablename2; 参考：https://www.jianshu.com/p/a8e259b973ef]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F06%2F07%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>hello world</category>
      </categories>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
</search>
