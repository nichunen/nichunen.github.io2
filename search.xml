<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[使用git进行项目版本管理]]></title>
    <url>%2F2019%2F07%2F09%2Flearn-git%2F</url>
    <content type="text"><![CDATA[本文以Apache Kylin为例，介绍使用git进行项目分支管理，以及常见开发中的问题及解决方法。 常见的分支类型 上图为截取的最近Apache Kylin项目的分支图，目前主要有以下几种分支类型： master分支 维护分支 feature分支 release分支 作用 最新的代码，随时会被更新 维护版本的代码，理论上只接受改动较小的enhancement和bug fix 为开发一些大改动的功能单拉的分支 用作版本发布的分支 例子 master 2.6.x engine-flink, kylin-on-druid v2.6.3-release, v-3.0.0-alpha-release 生命周期 从项目诞生到删除 从一个稳定的维护系列版本产生到该系列版本停止维护 从feature诞生到合并进入主分支 从准备发布到发布正式annunce 每一个版本release后，都会打上对应的tag，目前有kylin-2.6.3, kylin-3.0.0-alpha等 常用的git命令基本操作 查看 1234567891011121314151617git status #显示有变更的文件git log #显示当前分支的版本历史git log --stat #显示commit历史，以及每次commit发生变更的文件git blame [file] #显示指定文件是什么人在什么时间修改过 git diff #显示暂存区和工作区的差异git diff HEAD #显示工作区与当前分支最新commit之间的差异git show [commit] #显示某次提交的元数据和内容变化git show [commit]:[filename] #显示某次提交时，某个文件的内容git reflog #显示当前分支的最近几次提交 增加/删除文件 1234567891011git add [file1] #添加指定文件到暂存区git add [dir] #添加指定目录到暂存区，包括子目录git add . #添加当前目录的所有文件到暂存区git rm [file1] #删除工作区文件，并且将这次删除放入暂存区git rm --cached #[file] 停止追踪指定文件，但该文件会保留在工作区git mv &lt;file_original&gt; &lt;file_renamed&gt; #改名文件，并且将这个改名放入暂存区 代码提交 1234567891011git commit -m [message] #提交暂存区到仓库区git commit [file1] -m [message] #提交暂存区的指定文件到仓库区git commit -a #提交工作区自上次commit之后的变化，直接到仓库区git commit -v #提交时显示所有diff信息git commit --amend -m [message] #使用一次新的commit，替代上一次提交. 如果代码没有任何新变化，则用来改写上一次commit的提交信息git commit --amend [file1] #重做上一次commit，并包括指定文件的新变化 远程仓库 12345678910111213141516171819202122git clone [远程仓库地址] #从远程仓库克隆git fetch &lt;远程主机名&gt; &lt;分支名&gt; #更新远程仓库的所有变动，如git fetch origin master# 要更新所有分支，命令可以简写为 git fetchgit remote -v #显示所有远程仓库git remote show [remote] #显示某个远程仓库的信息git remote add [shortname] [url] #增加一个新的远程仓库，并命名git pull [remote] [branch] # 取回远程仓库的变化，并与本地分支合并git push &lt;远程主机名&gt; &lt;本地分支名&gt;:&lt;远程分支名&gt; #将本地分支的更新，推送到远程主机# 如果省略远程分支名，则表示将本地分支推送与之存在"追踪关系"的远程分支 git push origin master # 如果当前分支与远程分支之间存在追踪关系, 则本地分支和远程分支都可以省略 git push origin# 如果当前分支只有一个追踪分支，那么主机名都可以省略 git push# 如果当前分支与多个主机存在追踪关系，则可以使用-u选项指定一个默认主机，这样后面就可以不加任何参数使用git push# ！！！！慎用，有被杀了祭天的风险 ！！！！git push --force origin # 如果远程主机的版本比本地版本更新，推送时Git会报错，要求先在本地做git pull合并差异，然后再推送到远程主机。这时，如果你一定要推送，可以使用--force选项# ！！！！慎用，有被杀了祭天的风险 ！！！！ 标签管理 1234567891011121314151617git tag &lt;name&gt; #在当前分支打标签git tag #查看所有标签git show &lt;tagname&gt; #查看标签信息git tag -a &lt;tagname&gt; -m "" #可以指定标签信息git push origin &lt;tagname&gt; #推送一个本地标签git push origin --tags #推送全部未推送过的本地标签git tag -d &lt;tagname&gt; #删除一个本地标签git push origin:refs/tags/&lt;tagname&gt; #删除一个远程标签git checkout &lt;tagname&gt; #checkout到一个指定tag下对应的代码 分支管理 12345678910111213git checkout -b dev #创建并切换到dev分支git branch dev #创建分支git checkout dev #切换分支git branch #查看当前分支git merge dev #当前分支与dev分支合并git branch -d dev #删除dev分支 git push --set-upstream &lt;remote&gt; &lt;branch_name&gt; #将当前分支提交到远端 撤销 1234567891011121314151617181920git checkout [file] #恢复暂存区的指定文件到工作区git checkout [commit] [file] #恢复某个commit的指定文件到暂存区和工作区git checkout . #恢复暂存区的所有文件到工作区git reset [file] #重置暂存区的指定文件，与上一次commit保持一致，但工作区不变git reset --hard #重置暂存区与工作区，与上一次commit保持一致 git reset [commit] #重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变git reset --hard [commit] #重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致git reset --keep [commit] #重置当前HEAD为指定commit，但保持暂存区和工作区不变git revert [commit] #新建一个commit，用来撤销指定commit 后者的所有变化都将被前者抵消，并且应用到当前分支git stash #暂时将未提交的变化移除git stash pop #将在缓冲区的变动移入 理解HEADGit 中的 HEAD 可以理解为指针，指向当前仓库所处的分支。一般在有 git 管理的目录下打开 git 终端都能在当前路径的尾巴上，看到所处的分支名。 当使用 git checkout &lt; branch_name&gt; 切换分支时，HEAD 会移动到指定分支。 但是如果使用的是 git checkout &lt;commit id&gt;，即切换到指定的某一次提交，HEAD 就会处于 detached 状态（游离状态）。在这个基础上的提交会新开一个匿名分支。 常见案例如何提交一个Kylin的Pull Request Fork Apache Kylin项目，则会生成个人的Kylin项目，如https://github.com/nichunen/kylin.git Clone个人fork的项目到本地 git clone https://github.com/nichunen/kylin.git 基于master分支或feature分支新建一个分支，一般为Jira issue号 git checkout -b KYLIN-4042 基于上述分支进行开发 将改动commit到本地，并push到远程 在github打开apache/kylin项目，页面会自动提示创建PR 正确地进行rebaseRebase 实际上就是取出一系列的提交记录，“复制”它们，然后在另外一个地方逐个的放下去。Rebase 的优势就是可以创造更线性的提交历史。如果只允许使用 Rebase 的话，代码库的提交历史将会变得异常清晰。 如下图的项目，现在我们需要新建并切换到 bugFix 分支，进行提交一次，然后切换回 master 分支再提交一次。再次切换到 bugFix 分支，rebase master 上的改动。 命令大致如下 123456git checkout -b BugFixgit commitgit checkout mastergit commitgit checkout BugFixgit rebase master 如何快速向上移动一个/多个commit12git checkout master^git checkout master~&#123;number&#125; 如何进行cherry-pick如下图的分支结构，现在需要将三个分支中的C3,C4,C7提交记录复制到 master 上 git cherry-pick C3 C4 C7 使用rebase -i对commit进行合并与调整如下图项目的commit记录 现在需要将最上面的两个commit合并为一个commit，这里就用到利器rebase -i命令 1git rebase -i daee8c23fc740fc99a5be6a7fbc6dc8c78e3a46e 出现如下编辑页面 将第二个commit修改为s(squash)，这个commit会被合并至前一个commit 出现如下编辑页面，修改commit message如下 再执行git log，最近的commit即变更为一个合并的commit 执行git push -f强行提交 生成和合并patch将最上面的2个commit生成为2个patch文件 1git format-patch HEAD^^ 执行patch文件的代码修改 1git apply 0001-This-is-commit-1-2.patch 执行patch文件的代码修改并commit，并忽略空格 1git am -3 --ignore-whitespace 0001-This-is-commit-1-2.patch]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kylin的任务调度介绍]]></title>
    <url>%2F2019%2F03%2F30%2FKylin-s-job-scheduler%2F</url>
    <content type="text"><![CDATA[Kylin的任务调度介绍Kylin的任务调度模式采用的是生产者-消费者模式。 Kylin的构建、合并、优化segment任务的入口为EngineFactory的createBatchCubingJob, createBatchMergeJob, createBatchOptimizeJob方法。EngineFactory是一个工厂类，可以根据不同的配置选择不同的cube构建类型（Batch或者Streaming）和不同的cube构建算法（Batch Cube有不同的构建算法）。 Kylin的可执行任务都被表示为一个Executable对象，基本的UML图如下 Executable的核心方法是execute，每个实现类可以定义Job的具体执行逻辑。 1234567891011121314151617public interface Executable &#123; String getId(); String getName(); //定义Job的具体执行逻辑 //参数ExecutableContext Job的上下文 //结果ExecuteResult Job的状态和输出 ExecuteResult execute(ExecutableContext executableContext) throws ExecuteException; //获取Job运行状态 ExecutableState getStatus(); Output getOutput(); boolean isRunnable(); //获取Job的执行参数 Map&lt;String, String&gt; getParams();&#125; 抽象类AbstractExecutable实现了Executable接口，核心是实现了execute方法，为了清晰的定义每个Job的运行状态，AbstractExecutable将execute方法细化为onExecuteStart，doWork，onExecuteError，onExecuteFinished等阶段。其中execute方法修饰符为final，onExecuteStart，onExecuteError，onExecuteFinished方法修饰符为protected，doWork方法修饰符为protected abstract，用于子类根据自己的具体逻辑重写此方法。 execute方法的关键代码如下： 1234567891011121314public final ExecuteResult execute(ExecutableContext executableContext) throws ExecuteException &#123; //Job的状态从Ready 变为 Running onExecuteStart(executableContext); ... //不同的Job在这里实现具体逻辑 result = doWork(executableContext); ... if (exception != null) &#123; //Job的状态从Ready 变为 Error onExecuteError(exception, executableContext); &#125; //Job的状态从Ready 变为 Succeed or Error or Discard onExecuteFinished(result, executableContext); &#125; AbstractExecutable的具体直接实现类主要有ShellExecutable，HadoopShellExecutable，MapReduceExecutable，主要是根据自身的具体逻辑重写了doWork`方法。 ShellExecutable主要用来执行Shell 命令可以直接执行的Job，像计算Hive表行数等Job。 HadoopShellExecutable主要用来执行依赖Hadoop环境且用Shell执行的Job，像建字典，建立HBase表，Bulkload HFile等Job。 MapReduceExecutable主要用来执行MapReduce类型的Job，像计算列基数，计算Cuboid， 生成HFile等Job。 前面提到是CubingJob将构建cube的每一步job串了起来，其实CubingJob继承了DefaultChainedExecutable，DefaultChainedExecutable类继承了 AbstractExecutable类并实现了ChainedExecutable接口。CubingJob 主要是为Job关联了cube和segment的相关信息，串连所有Job的任务都是DefaultChainedExecutable类实现的。 123456public interface ChainedExecutable extends Executable &#123; //获取所有子Job List&lt;? extends AbstractExecutable&gt; getTasks(); //添加子Job void addTask(AbstractExecutable executable);&#125; 之前提到在生成Job的时候，CubingJob通过addTask方法将所有子Job串连了起来。那么DefaultChainedExecutable到底是如何串连起所有子Job呢？关键在其重写的doWork方法里： 123456789101112131415161718protected ExecuteResult doWork(ExecutableContext context) throws ExecuteException &#123; List&lt;? extends Executable&gt; executables = getTasks();//获取所有子Job for (int i = 0; i &lt; executables.size(); ++i) &#123; Executable subTask = executables.get(i); ExecutableState state = subTask.getStatus(); if (state == ExecutableState.RUNNING) &#123; //子Job正在执行，等待它完成 break; &#125; else if (state == ExecutableState.ERROR) &#123; //子Job执行失败，抛出异常 &#125; if (subTask.isRunnable()) &#123; //每个Job在初始化后是Ready状态，所以isRunnable()是True，当子Job是Ready状态时，就开始执行。 return subTask.execute(context); &#125; &#125; return new ExecuteResult(ExecuteResult.State.SUCCEED, null); &#125; DefaultChainedExecutable 也重写了onExecuteFinished方法，来根据所有子Job的状态更新整个Job的最终状态。 Kylin默认通过DefaultScheduler进行任务的调度，其核心逻辑是十分简单的，有两个线程池，一个线程池用来抓取所有Job的状态信息，一个线程池来执行具体的Job。基本的逻辑过程如下： 参考：https://blog.bcmeng.com/post/kylin-job.html]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kylin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kylin的元数据-任务]]></title>
    <url>%2F2019%2F03%2F30%2FKylin-s-metadata-execute%2F</url>
    <content type="text"><![CDATA[Kyin任务元数据基本介绍在Kylin的元数据基础一文中介绍了kylin元数据的一些源码级基础知识。使用 Kylin生成每执行一个构建或合并任务，都会在Monitor页面生成一个job Kylin使用存于resource store的metadata来实现元数据的持久化保存。 对于任务元数，Kylin使用了2中类型的元数据，分别为execute和execute_output execute：存储与表示任务的执行计划 execute_output：存储与表示任务的执行结果 每个任务仅有一个execute元数据，会有(step_num + 1)个execute_output元数据，如下图所示为一dump下来的某个任务的元数据 execute的基本元数据类为org.apache.kylin.job.dao.ExecutablePO，代码为 1234567891011121314151617181920public class ExecutablePO extends RootPersistentEntity &#123; // 任务名 @JsonProperty("name") private String name; // 子任务列表 @JsonProperty("tasks") private List&lt;ExecutablePO&gt; tasks; // 任务类型 @JsonProperty("type") private String type; // 任务参数 @JsonProperty("params") private Map&lt;String, String&gt; params = Maps.newHashMap(); //....&#125; 生成的json元数据文件示例如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&#123; "uuid": "99b80122-b59b-bbdb-5f22-26d1d9270220", "last_modified": 1555246404591, "version": "2.6.0.20500", "name": "BUILD CUBE - kylin_sales_cube - 20120101000000_20190430185000 - GMT+08:00 2019-04-14 20:53:24", "tasks": [ &#123; "uuid": "99b80122-b59b-bbdb-5f22-26d1d9270220-00", "last_modified": 0, "version": "2.6.0.20500", "name": "Create Intermediate Flat Hive Table", "tasks": null, "type": "org.apache.kylin.source.hive.CreateFlatHiveTableStep", "params": &#123; "HiveInit": "USE default; ", "HiveRedistributeData": "DROP TABLE IF EXISTS kylin_intermediate_kylin_sales_cube_86f98364_1485_001b_8fc5_18ec50930a14; CREATE EXTERNAL TABLE IF NOT EXISTS kylin_intermediate_kylin_sales_cube_86f98364_1485_001b_8fc5_18ec50930a14 ( KYLIN_SALES_TRANS_ID bigint ,KYLIN_SALES_PART_DT date ,KYLIN_SALES_LEAF_CATEG_ID bigint ,KYLIN_SALES_LSTG_SITE_ID int ,KYLIN_CATEGORY_GROUPINGS_META_CATEG_NAME string ,KYLIN_CATEGORY_GROUPINGS_CATEG_LVL2_NAME string ,KYLIN_CATEGORY_GROUPINGS_CATEG_LVL3_NAME string ,KYLIN_SALES_LSTG_FORMAT_NAME string ,KYLIN_SALES_SELLER_ID bigint ,KYLIN_SALES_BUYER_ID bigint ,BUYER_ACCOUNT_ACCOUNT_BUYER_LEVEL int ,SELLER_ACCOUNT_ACCOUNT_SELLER_LEVEL int ,BUYER_ACCOUNT_ACCOUNT_COUNTRY string ,SELLER_ACCOUNT_ACCOUNT_COUNTRY string ,BUYER_COUNTRY_NAME string ,SELLER_COUNTRY_NAME string ,KYLIN_SALES_OPS_USER_ID string ,KYLIN_SALES_OPS_REGION string ,KYLIN_SALES_PRICE decimal(19,4) ) STORED AS SEQUENCEFILE LOCATION 'hdfs://sandbox.hortonworks.com:8020/Users/nichunen/Downloads/template/kylin-99b80122-b59b-bbdb-5f22-26d1d9270220/kylin_intermediate_kylin_sales_cube_86f98364_1485_001b_8fc5_18ec50930a14'; ALTER TABLE kylin_intermediate_kylin_sales_cube_86f98364_1485_001b_8fc5_18ec50930a14 SET TBLPROPERTIES('auto.purge'='true'); INSERT OVERWRITE TABLE `kylin_intermediate_kylin_sales_cube_86f98364_1485_001b_8fc5_18ec50930a14` SELECT `KYLIN_SALES`.`TRANS_ID` as `KYLIN_SALES_TRANS_ID` ,`KYLIN_SALES`.`PART_DT` as `KYLIN_SALES_PART_DT` ,`KYLIN_SALES`.`LEAF_CATEG_ID` as `KYLIN_SALES_LEAF_CATEG_ID` ,`KYLIN_SALES`.`LSTG_SITE_ID` as `KYLIN_SALES_LSTG_SITE_ID` ,`KYLIN_CATEGORY_GROUPINGS`.`META_CATEG_NAME` as `KYLIN_CATEGORY_GROUPINGS_META_CATEG_NAME` ,`KYLIN_CATEGORY_GROUPINGS`.`CATEG_LVL2_NAME` as `KYLIN_CATEGORY_GROUPINGS_CATEG_LVL2_NAME` ,`KYLIN_CATEGORY_GROUPINGS`.`CATEG_LVL3_NAME` as `KYLIN_CATEGORY_GROUPINGS_CATEG_LVL3_NAME` ,`KYLIN_SALES`.`LSTG_FORMAT_NAME` as `KYLIN_SALES_LSTG_FORMAT_NAME` ,`KYLIN_SALES`.`SELLER_ID` as `KYLIN_SALES_SELLER_ID` ,`KYLIN_SALES`.`BUYER_ID` as `KYLIN_SALES_BUYER_ID` ,`BUYER_ACCOUNT`.`ACCOUNT_BUYER_LEVEL` as `BUYER_ACCOUNT_ACCOUNT_BUYER_LEVEL` ,`SELLER_ACCOUNT`.`ACCOUNT_SELLER_LEVEL` as `SELLER_ACCOUNT_ACCOUNT_SELLER_LEVEL` ,`BUYER_ACCOUNT`.`ACCOUNT_COUNTRY` as `BUYER_ACCOUNT_ACCOUNT_COUNTRY` ,`SELLER_ACCOUNT`.`ACCOUNT_COUNTRY` as `SELLER_ACCOUNT_ACCOUNT_COUNTRY` ,`BUYER_COUNTRY`.`NAME` as `BUYER_COUNTRY_NAME` ,`SELLER_COUNTRY`.`NAME` as `SELLER_COUNTRY_NAME` ,`KYLIN_SALES`.`OPS_USER_ID` as `KYLIN_SALES_OPS_USER_ID` ,`KYLIN_SALES`.`OPS_REGION` as `KYLIN_SALES_OPS_REGION` ,`KYLIN_SALES`.`PRICE` as `KYLIN_SALES_PRICE` FROM `DEFAULT`.`KYLIN_SALES` as `KYLIN_SALES` INNER JOIN `DEFAULT`.`KYLIN_CAL_DT` as `KYLIN_CAL_DT` ON `KYLIN_SALES`.`PART_DT` = `KYLIN_CAL_DT`.`CAL_DT` INNER JOIN `DEFAULT`.`KYLIN_CATEGORY_GROUPINGS` as `KYLIN_CATEGORY_GROUPINGS` ON `KYLIN_SALES`.`LEAF_CATEG_ID` = `KYLIN_CATEGORY_GROUPINGS`.`LEAF_CATEG_ID` AND `KYLIN_SALES`.`LSTG_SITE_ID` = `KYLIN_CATEGORY_GROUPINGS`.`SITE_ID` INNER JOIN `DEFAULT`.`KYLIN_ACCOUNT` as `BUYER_ACCOUNT` ON `KYLIN_SALES`.`BUYER_ID` = `BUYER_ACCOUNT`.`ACCOUNT_ID` INNER JOIN `DEFAULT`.`KYLIN_ACCOUNT` as `SELLER_ACCOUNT` ON `KYLIN_SALES`.`SELLER_ID` = `SELLER_ACCOUNT`.`ACCOUNT_ID` INNER JOIN `DEFAULT`.`KYLIN_COUNTRY` as `BUYER_COUNTRY` ON `BUYER_ACCOUNT`.`ACCOUNT_COUNTRY` = `BUYER_COUNTRY`.`COUNTRY` INNER JOIN `DEFAULT`.`KYLIN_COUNTRY` as `SELLER_COUNTRY` ON `SELLER_ACCOUNT`.`ACCOUNT_COUNTRY` = `SELLER_COUNTRY`.`COUNTRY` WHERE 1=1 AND (`KYLIN_SALES`.`PART_DT` &gt;= '2012-01-01' AND `KYLIN_SALES`.`PART_DT` &lt; '2019-04-30') ; ", "cubeName": "kylin_sales_cube" &#125; &#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125; ], "type": "org.apache.kylin.engine.mr.CubingJob", "params": &#123; "submitter": "ADMIN", "envName": "DEV", "segmentId": "86f98364-1485-001b-8fc5-18ec50930a14", "notify_list": "", "projectName": "learn_kylin", "jobType": "BUILD", "cubeName": "kylin_sales_cube", "segmentName": "20120101000000_20190430185000" &#125;&#125; execute_output的基本元数据类为org.apache.kylin.job.dao.ExecutableOutputPO，代码为 1234567891011public class ExecutableOutputPO extends RootPersistentEntity &#123; @JsonProperty("content") private String content; @JsonProperty("status") private String status = "READY"; @JsonProperty("info") private Map&lt;String, String&gt; info = Maps.newHashMap();&#125; 生成的全局任务json元数据文件示例如下： 1234567891011121314&#123; "uuid": "99b80122-b59b-bbdb-5f22-26d1d9270220", "last_modified": 1555246411812, "version": "2.6.0.20500", "content": "org.apache.kylin.job.exception.ExecuteException: java.io.IOException: OS command error exit with return code: 127, error message: /bin/bash: hive: command not foundThe command is: hive -e \"", "status": "ERROR", "info": &#123; "startTime": "1555246411762", "buildInstance": "62641@GggdeMacBook-Pro.local", "endTime": "1555246411811" &#125;&#125; 生成的子任务json元数据文件示例如下： 12345678910111213&#123; "uuid": "99b80122-b59b-bbdb-5f22-26d1d9270220-00", "last_modified": 1555246411809, "version": "2.6.0.20500", "content": "java.io.IOException: OS command error exit with return code: 127, error message: /bin/bash: hive: command not foundThe command is: hive -e \"", "status": "ERROR", "info": &#123; "startTime": "1555246411769", "endTime": "1555246411807" &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kylin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kylin的元数据基础]]></title>
    <url>%2F2019%2F03%2F30%2FKylin-s-metadata-base%2F</url>
    <content type="text"><![CDATA[Apache Kylin元数据基础Apache Kylin的元数据包括 立方体描述（cube description）、立方体实例（cube instance）、项目（project）、模型描述（model description）、作业（job）、表（table）、字典(dictionary)等，参见： Kylin核心概念。在kylin集群中至关重要，假如元数据丢失，kylin集群将无法工作。 Kylin源码定义了ResourceStore这一抽象类来定义，其中定义了各种对元数据增删改查的方法，主要方法如： 1234567891011121314151617// 根据kylinConfig获得对应的ResourceStoreResourceStore getStore(KylinConfig kylinConfig)// 根据目录获取所有元数据路径NavigableSet&lt;String&gt; listResources(String folderPath)// 路径resPath下是否存在元数据boolean exists(String resPath) // 根据路径resPath及序列化器serializer返回对应的元数据&lt;T extends RootPersistentEntity&gt; T getResource(String resPath, Serializer&lt;T&gt; serializer) // 根据路径resPath写入元数据 public &lt;T extends RootPersistentEntity&gt; long putResource(String resPath, T obj, long ts, Serializer&lt;T&gt; serializer) // 删除路径resPath下的元数据void deleteResource(String resPath) ResourceStore类的主要子类继承关系图如下： Kylin支持多种数据源格式，如Hbase、Jdbc等，只要继承ResourceStore类并实现必须的Impl方法即可。 Kylin的所有序列化的元数据都继承自RootPersistentEntity抽象类，该类及其子类都使用了jackson的JsonAutoDetect注解。 例如CubeDesc部分代码如下： 1234567891011121314151617181920212223242526272829@SuppressWarnings(&quot;serial&quot;)@JsonAutoDetect(fieldVisibility = Visibility.NONE, getterVisibility = Visibility.NONE, isGetterVisibility = Visibility.NONE, setterVisibility = Visibility.NONE)public class CubeDesc extends RootPersistentEntity implements IEngineAware &#123; //... @JsonProperty(&quot;name&quot;) private String name; @JsonProperty(&quot;is_draft&quot;) private boolean isDraft; @JsonProperty(&quot;model_name&quot;) private String modelName; @JsonProperty(&quot;description&quot;) private String description; @JsonProperty(&quot;null_string&quot;) private String[] nullStrings; @JsonProperty(&quot;dimensions&quot;) private List&lt;DimensionDesc&gt; dimensions; @JsonProperty(&quot;measures&quot;) private List&lt;MeasureDesc&gt; measures; @JsonProperty(&quot;dictionaries&quot;) @JsonInclude(JsonInclude.Include.NON_NULL) private List&lt;DictionaryDesc&gt; dictionaries; @JsonProperty(&quot;rowkey&quot;) private RowKeyDesc rowkey; @JsonProperty(&quot;hbase_mapping&quot;) private HBaseMappingDesc hbaseMapping; @JsonProperty(&quot;aggregation_groups&quot;) private List&lt;AggregationGroup&gt; aggregationGroups; // ...&#125; 对应在元数据库的json数据如下： 12345678910111213141516&#123; "uuid":"0ef9b7a8-3929-4dff-b59d-2100aadc8dbf", "last_modified":1451468470824, "version":"%default_version%", "name":"kylin_sales_cube", "is_draft":false, "model_name":"kylin_sales_model", "description":"", "null_string":null, "dimensions":[], "measures":[], "rowkey":&#123;&#125;, "hbase_mapping":&#123;&#125;, "aggregation_groups":[], ...&#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kylin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OSI七层模型的通俗理解]]></title>
    <url>%2F2019%2F03%2F12%2Fosi-tcpip%2F</url>
    <content type="text"><![CDATA[OSI七层模型的通俗理解 以你和你女朋友以书信的方式进行通信为例。 物理层：运输工具，比如火车、汽车 数据链路层：相当于货物核对单，表明里面有些什么东西，接受的时候确认一下是否正确（CRC检验） 网络层：相当于邮政局或快递公司地址（IP地址），能正确到达对方 传输层：信封（TCP协议是挂号信，是可靠的；UDP协议是平信，尽力送到对方，不保证一点送到对方） 会话层：相当于邮票，优质邮票寄一封信，相当与一个会话 表示层：你用普通话还是用方言？或者是英语？ 应用层：你可以说你的内容了，可以说是你爱她，也可以说你恨她。]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用gogs安装本地git服务(Centos环境)]]></title>
    <url>%2F2019%2F03%2F11%2Fgogs-install%2F</url>
    <content type="text"><![CDATA[使用gogs安装本地git服务(Centos环境)安装数据库Gogs支持MySQL、PostgreSQL、SQLite3、TiDB。安装过程这里不赘述，我们使用的是MySQL。 安装git1yum install -y git 添加git用户（gogs期望用git用户操作）1sudo useradd git 下载并安装根据自己的linux版本在 https://dl.gogs.io/ 下载安装包，linux版本根据uname -a查看 1wget https://dl.gogs.io/0.11.86/gogs_0.11.86_linux_amd64.tar.gz 解压 1tar -zxvf gogs_0.11.86_linux_amd64.tar.gz 进入解压的目录，执行 12cd gogs/./gogs web 打开${ip}:3000页面，出现如下配置页面 配置完数据库、端口、用户信息等 运行gogsControl+C暂停当前gogs进程，后台运行gogs 1nohup ./gogs web &amp; 打开${ip}:3000页面，注册后登陆]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive 常用命令]]></title>
    <url>%2F2019%2F03%2F10%2Fhive-cli%2F</url>
    <content type="text"><![CDATA[Hive常用命令创建表： 1CREATE TABLE pokes (foo INT, bar STRING); 创建一个新表，结构与其他一样： 1create table new_table like records; 创建分区表： 1create table logs(ts bigint,line string) partitioned by (dt String,country String); 加载分区表数据： 1load data local inpath '/home/Hadoop/input/hive/partitions/file1' into table logs partition (dt='2001-01-01',country='GB'); 展示表中有多少分区： 1show partitions logs; 展示所有表： 1SHOW TABLES; 显示表的结构信息： 1DESCRIBE invites; 更新表的名称： 1ALTER TABLE source RENAME TO target; 添加新一列： 1ALTER TABLE invites ADD COLUMNS (new_col2 INT COMMENT 'a comment'); 添加新的一行： 1INSERT INTO test(name,pwd,createdate) values('name1','pwd1','2017-06-20 14:14:09'); 删除表： 1DROP TABLE records; 删除表中数据，但要保持表的结构定义： 1TRUNCATE TABLE table_name 从本地文件加载数据： 1LOAD DATA LOCAL INPATH '/home/hadoop/input/ncdc/micro-tab/sample.txt' OVERWRITE INTO TABLE records; 显示所有函数： 1show functions; 查看函数用法： 1describe function substr; 内连接： 1SELECT sales., things. FROM sales JOIN things ON (sales.id = things.id); 查看hive为某个查询使用多少个MapReduce作业: 1Explain SELECT sales., things. FROM sales JOIN things ON (sales.id = things.id); 外连接： 1SELECT sales., things. FROM sales LEFT OUTER JOIN things ON (sales.id = things.id); 创建视图： 1CREATE VIEW valid_records AS SELECT * FROM records2 WHERE temperature !=9999; INSERT OVERWRITE： 1INSERT OVERWRITE table tablename1 select a, b, c from tablename2; 参考：https://www.jianshu.com/p/a8e259b973ef]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F06%2F07%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>hello world</category>
      </categories>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
</search>
